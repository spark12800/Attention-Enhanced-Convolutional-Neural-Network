{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Maps Probing\n",
        "To reveal how functions modulated weight distributions, quantitative metrics (top-1/top-5 accuracy for classification, average precision for localisation) were complemented by qualitative analysis of attention maps.\n",
        "\n",
        "## 0. Set-up"
      ],
      "metadata": {
        "id": "GdWh9Yahl-H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sparsemax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyaivtg9nTDu",
        "outputId": "151925af-b214-4e63-8d4a-b4157b8ae4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparsemax\n",
            "  Downloading sparsemax-0.1.9-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from sparsemax) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->sparsemax)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->sparsemax)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->sparsemax)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->sparsemax)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->sparsemax)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->sparsemax)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->sparsemax)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->sparsemax)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->sparsemax)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->sparsemax)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->sparsemax) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->sparsemax) (3.0.2)\n",
            "Downloading sparsemax-0.1.9-py2.py3-none-any.whl (5.5 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sparsemax\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sparsemax-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sparsemax import Sparsemax\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.stats import gmean\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Wkj0xFovZplC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "rRcKkumLnRkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset Preparation\n",
        "\n",
        "Data loader for CIFAR-10 with resizing to 224x224 to match pretrained ResNet input expectations:"
      ],
      "metadata": {
        "id": "CEu_fWS1mCdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "bGnPlCjUnSTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building Attention Enhanced Convolutional Neural Network (CNN) models"
      ],
      "metadata": {
        "id": "z6Z0Yf8ymDjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        return self.sigmoid(x)"
      ],
      "metadata": {
        "id": "gxLBZDzITSdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Channel Attention with configurable activation\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, activation_type='sigmoid'):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes//ratio, 1, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes//ratio, in_planes, 1, bias=False)\n",
        "        self.activation_type = activation_type\n",
        "\n",
        "        # Activation parameters\n",
        "        self.scale = nn.Parameter(torch.ones(1)) if activation_type == 'scaled_tanh' else None\n",
        "        self.temp = nn.Parameter(torch.ones(1)) if activation_type == 'parametric_sigmoid' else None\n",
        "        self.sparsemax = Sparsemax(dim=1) if activation_type == 'sparsemax' else None\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
        "        out = avg_out + max_out\n",
        "\n",
        "        if self.activation_type == 'softmax':\n",
        "            return self.softmax(out.view(out.size(0), -1)).view_as(out)\n",
        "        elif self.activation_type == 'sparsemax':\n",
        "            return self.sparsemax(out.view(out.size(0), -1)).view_as(out)\n",
        "        elif self.activation_type == 'scaled_tanh':\n",
        "            return (torch.tanh(self.scale * out) + 1) / 2\n",
        "        elif self.activation_type == 'parametric_sigmoid':\n",
        "            return self.sigmoid(out / self.temp)\n",
        "        elif self.activation_type == 'swish':\n",
        "            return out * torch.sigmoid(out)\n",
        "        else:  # sigmoid\n",
        "            return self.sigmoid(out)"
      ],
      "metadata": {
        "id": "kPyHm1iRTVhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self, planes, ratio=16, channel_activation='sigmoid'):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(planes, ratio, channel_activation)\n",
        "        self.sa = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.ca(x)\n",
        "        x = x * self.sa(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "juXE21lzTZj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Building\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### ResNet with CBAM:\n",
        "\n",
        "- Custom ResNet18 wrapper that conditionally adds CBAM at different depths"
      ],
      "metadata": {
        "id": "tpFUJWn-mKnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18_CBAM(nn.Module):\n",
        "    def __init__(self, attention_position=\"middle\", num_classes=10, channel_activation='sigmoid'):\n",
        "        super(ResNet18_CBAM, self).__init__()\n",
        "        base = models.resnet18(pretrained=True)\n",
        "        self.stem = nn.Sequential(base.conv1, base.bn1, base.relu, base.maxpool)\n",
        "        self.layer1 = base.layer1\n",
        "        self.layer2 = base.layer2\n",
        "        self.layer3 = base.layer3\n",
        "        self.layer4 = base.layer4\n",
        "        self.avgpool = base.avgpool\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Add CBAM modules\n",
        "        self.attention_position = attention_position\n",
        "        if attention_position == \"early\":\n",
        "            self.cbam1 = CBAM(64, channel_activation=channel_activation)\n",
        "        elif attention_position == \"middle\":\n",
        "            self.cbam2 = CBAM(128, channel_activation=channel_activation)\n",
        "        elif attention_position == \"late\":\n",
        "            self.cbam3 = CBAM(512, channel_activation=channel_activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        if self.attention_position == \"early\":\n",
        "            x = self.layer1(x)\n",
        "            x = self.cbam1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "            x = self.layer4(x)\n",
        "        elif self.attention_position == \"middle\":\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.cbam2(x)\n",
        "            x = self.layer3(x)\n",
        "            x = self.layer4(x)\n",
        "        elif self.attention_position == \"late\":\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "            x = self.layer4(x)\n",
        "            x = self.cbam3(x)\n",
        "        else:  # no attention\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "            x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "-xkNzTgdTcjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metric\n",
        "\n",
        "* Entropy: Quantify the uncertainty and spread of attention. Lower entropy indicates sharper focus, linked to interpretability in prior work.\n",
        "\n",
        "* Mean Attention Value: Reflect global activation strength.\n",
        "\n",
        "* Focus Area Ratio: Measure spatial concentration of high-attention regions, pixels exceeding 50% of maximum attention, inspired by saliency detection.\n",
        "\n",
        "* Gini Coefficient: Evaluate inequality in attention weight distribution, where values closer to 1 indicate extreme concentration and values near 0 reflect uniformity."
      ],
      "metadata": {
        "id": "JphAM5nJmSyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_coefficient(x):\n",
        "    \"\"\"Compute Gini coefficient (measure of sparsity).\"\"\"\n",
        "    x = np.sort(x)\n",
        "    n = len(x)\n",
        "    cumx = np.cumsum(x)\n",
        "    return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
        "\n",
        "def compute_metrics(att_weights, test_acc):\n",
        "    \"\"\"Calculate attention distribution metrics.\"\"\"\n",
        "    return {\n",
        "        'activation': 'sigmoid',  # Placeholder, will be overwritten\n",
        "        'avg_score': np.mean(att_weights),\n",
        "        'focus_%': 100 * np.mean(att_weights > np.mean(att_weights)),\n",
        "        'sparsity': 1 - np.count_nonzero(att_weights) / len(att_weights),\n",
        "        'gini': gini_coefficient(att_weights),\n",
        "        'test_acc': test_acc\n",
        "    }\n",
        "\n",
        "def plot_kde_comparison(all_weights):\n",
        "    \"\"\"Plot KDE for all activation types.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for act, weights in all_weights.items():\n",
        "        sns.kdeplot(weights, fill=True, label=act)\n",
        "    plt.title(\"Attention Weight Distributions\")\n",
        "    plt.xlabel(\"Attention Score\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_radar_chart(metrics_df):\n",
        "    \"\"\"Radar plot comparing metrics across activations.\"\"\"\n",
        "    categories = ['avg_score', 'focus_%', 'sparsity', 'gini', 'test_acc']\n",
        "    N = len(categories)\n",
        "    angles = [n / N * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'polar': True})\n",
        "    for _, row in metrics_df.iterrows():\n",
        "        values = row[categories].tolist()\n",
        "        values += values[:1]\n",
        "        ax.plot(angles, values, label=row['activation'])\n",
        "        ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(categories)\n",
        "    ax.set_title(\"Activation Mechanism Comparison\", size=16)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "X7Ppa7glToPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training and Evaluation\n",
        "\n",
        "Includes Top-1, Top-5, Average Precision and gradient flow plot & logging alpha values"
      ],
      "metadata": {
        "id": "_6pt_kAmmRGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation Functions\n",
        "def train_model(model, train_loader, test_loader, num_epochs=10, lr=0.001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss/len(train_loader)\n",
        "        train_acc = 100.*correct/total\n",
        "\n",
        "        # Validation\n",
        "        test_acc, test_loss = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    return model"
      ],
      "metadata": {
        "id": "89txMCllTfI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return 100.*correct/total, running_loss/len(test_loader)"
      ],
      "metadata": {
        "id": "cHNb3YLUTi0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Visualization\n",
        "def visualize_attention(model, test_loader, channel_activation):\n",
        "    model.eval()\n",
        "    images, _ = next(iter(test_loader))\n",
        "    img = images[0].unsqueeze(0).to(device)\n",
        "\n",
        "    attention_weights = []\n",
        "    def hook_fn(module, input, output):\n",
        "        attention_weights.append(output.detach().cpu().squeeze())  # Shape: [num_channels]\n",
        "\n",
        "    # Register hook\n",
        "    if model.attention_position == \"early\":\n",
        "        handle = model.cbam1.ca.register_forward_hook(hook_fn)\n",
        "    elif model.attention_position == \"middle\":\n",
        "        handle = model.cbam2.ca.register_forward_hook(hook_fn)\n",
        "    elif model.attention_position == \"late\":\n",
        "        handle = model.cbam3.ca.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(img)\n",
        "    handle.remove()\n",
        "\n",
        "    return attention_weights[0].numpy()  # Return as numpy array"
      ],
      "metadata": {
        "id": "1e3jg5K7T5Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all channel activation types to test\n",
        "channel_activation_types = ['sigmoid', 'softmax', 'sparsemax', 'scaled_tanh','parametric_sigmoid','swish']\n",
        "\n",
        "# Dictionary to store attention weights and metrics\n",
        "all_weights = {}\n",
        "results = []\n",
        "\n",
        "for channel_act in channel_activation_types:\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"Training with Channel Activation: {channel_act.upper()}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNet18_CBAM(attention_position=\"late\", channel_activation=channel_act).to(device)\n",
        "    trained_model = train_model(model, train_loader, test_loader, num_epochs=5, lr=0.001)\n",
        "\n",
        "    # Get attention weights and test accuracy\n",
        "    att_weights = visualize_attention(trained_model, test_loader, channel_act)\n",
        "    test_acc = evaluate_model(trained_model, test_loader, nn.CrossEntropyLoss())[0]\n",
        "\n",
        "    # Store results\n",
        "    all_weights[channel_act] = att_weights\n",
        "    metrics = compute_metrics(att_weights, test_acc)\n",
        "    metrics['activation'] = channel_act\n",
        "    results.append(metrics)\n",
        "\n",
        "    # Cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Metrics Summary ===\")\n",
        "print(results_df.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_qtqzO5Q_L6",
        "outputId": "5f7c566b-e603-4e84-ff10-7954ae70b327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "========================================\n",
            "Training with Channel Activation: SIGMOID\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:29<00:00,  8.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.5564, Train Acc: 80.99% | Test Loss: 0.4785, Test Acc: 83.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [01:29<00:00,  8.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.3190, Train Acc: 89.00% | Test Loss: 0.3170, Test Acc: 89.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [01:30<00:00,  8.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.2193, Train Acc: 92.47% | Test Loss: 0.3117, Test Acc: 89.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [01:29<00:00,  8.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.1511, Train Acc: 94.77% | Test Loss: 0.3264, Test Acc: 89.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [01:29<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.1177, Train Acc: 95.92% | Test Loss: 0.3258, Test Acc: 89.91%\n",
            "\n",
            "========================================\n",
            "Training with Channel Activation: SOFTMAX\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.5943, Train Acc: 42.15% | Test Loss: 1.3098, Test Acc: 49.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [01:29<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 1.2403, Train Acc: 52.56% | Test Loss: 1.3498, Test Acc: 52.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [01:29<00:00,  8.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.9842, Train Acc: 64.32% | Test Loss: 0.8150, Test Acc: 71.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [01:29<00:00,  8.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.7216, Train Acc: 76.40% | Test Loss: 0.6294, Test Acc: 80.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.5547, Train Acc: 82.80% | Test Loss: 0.5261, Test Acc: 83.48%\n",
            "\n",
            "========================================\n",
            "Training with Channel Activation: SPARSEMAX\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:31<00:00,  8.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.8539, Train Acc: 28.05% | Test Loss: 1.6437, Test Acc: 33.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [01:31<00:00,  8.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 1.6634, Train Acc: 32.97% | Test Loss: 1.6093, Test Acc: 35.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [01:31<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 1.5410, Train Acc: 39.03% | Test Loss: 1.4755, Test Acc: 41.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [01:31<00:00,  8.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 1.4543, Train Acc: 42.40% | Test Loss: 1.4041, Test Acc: 43.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [01:31<00:00,  8.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 1.3814, Train Acc: 43.93% | Test Loss: 1.3108, Test Acc: 46.48%\n",
            "\n",
            "========================================\n",
            "Training with Channel Activation: SCALED_TANH\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:29<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.5649, Train Acc: 80.82% | Test Loss: 0.4817, Test Acc: 83.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [01:29<00:00,  8.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.3181, Train Acc: 89.07% | Test Loss: 0.3841, Test Acc: 87.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [01:29<00:00,  8.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.2219, Train Acc: 92.25% | Test Loss: 0.3726, Test Acc: 88.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [01:29<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.1594, Train Acc: 94.49% | Test Loss: 0.3447, Test Acc: 89.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.1148, Train Acc: 95.94% | Test Loss: 0.3499, Test Acc: 89.19%\n",
            "\n",
            "========================================\n",
            "Training with Channel Activation: PARAMETRIC_SIGMOID\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.5686, Train Acc: 80.79% | Test Loss: 0.4698, Test Acc: 83.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.3159, Train Acc: 89.37% | Test Loss: 0.3845, Test Acc: 87.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [01:29<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.2189, Train Acc: 92.47% | Test Loss: 0.3481, Test Acc: 88.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [01:29<00:00,  8.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.1536, Train Acc: 94.79% | Test Loss: 0.3236, Test Acc: 89.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [01:29<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.1193, Train Acc: 95.91% | Test Loss: 0.3054, Test Acc: 90.47%\n",
            "\n",
            "========================================\n",
            "Training with Channel Activation: SWISH\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:29<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.6116, Train Acc: 79.39% | Test Loss: 0.4913, Test Acc: 83.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.3428, Train Acc: 88.30% | Test Loss: 0.3905, Test Acc: 86.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [01:29<00:00,  8.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.2378, Train Acc: 91.91% | Test Loss: 0.3628, Test Acc: 88.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [01:29<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.1723, Train Acc: 94.13% | Test Loss: 0.4249, Test Acc: 87.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [01:29<00:00,  8.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.1368, Train Acc: 95.31% | Test Loss: 0.3295, Test Acc: 89.54%\n",
            "\n",
            "=== Metrics Summary ===\n",
            "| activation         |   avg_score |   focus_% |   sparsity |     gini |   test_acc |\n",
            "|:-------------------|------------:|----------:|-----------:|---------:|-----------:|\n",
            "| sigmoid            |  0.645785   | 64.4531   |  0         | 0.354157 |      89.91 |\n",
            "| softmax            |  0.00195312 |  0.585938 |  0         | 0.996395 |      83.48 |\n",
            "| sparsemax          |  0.00195312 |  0.195312 |  0.998047  | 0.998047 |      46.48 |\n",
            "| scaled_tanh        |  0.619784   | 61.7188   |  0.0878906 | 0.379701 |      89.62 |\n",
            "| parametric_sigmoid |  0.610935   | 60.7422   |  0         | 0.388816 |      90.47 |\n",
            "| swish              |  1.27774    | 25.1953   |  0         | 0.896633 |      89.54 |\n"
          ]
        }
      ]
    }
  ]
}