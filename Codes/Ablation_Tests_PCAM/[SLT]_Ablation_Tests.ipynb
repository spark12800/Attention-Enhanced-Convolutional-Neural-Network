{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71aa3849",
      "metadata": {
        "id": "71aa3849"
      },
      "source": [
        "# Parametric Channel Attention Module (PCAM) Experiment\n",
        "PCAM is inspired by the Convolutional Block Attention Module (CBAM), but enhances it by introducing learnable parameters within the activation functions and adaptively combining average and max pooling outputs in the channel attention mechanism.\n",
        "\n",
        "PCAM has been built based on the extensive experiment below:  \n",
        "\n",
        "\n",
        "\n",
        "## 0. Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "giMf36lV9OET",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMf36lV9OET",
        "outputId": "8ee0805d-bfaf-43b3-ce57-83747dfa6c49"
      },
      "outputs": [],
      "source": [
        "pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8aa7f815",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.23.0)\n",
            "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "75jrFlSnVmq0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75jrFlSnVmq0",
        "outputId": "d1e9a739-02aa-4524-de6a-d7a5daaf43b7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import all necessary libraries for model building, training,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# evaluation, visualization, and dataset preparation.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m                     \u001b[38;5;66;03m# PyTorch core\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m            \u001b[38;5;66;03m# PyTorch neural network module\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m      \u001b[38;5;66;03m# PyTorch optimization algorithms\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries for model building, training,\n",
        "# evaluation, visualization, and dataset preparation.\n",
        "\n",
        "import torch                     # PyTorch core\n",
        "import torch.nn as nn            # PyTorch neural network module\n",
        "import torch.optim as optim      # PyTorch optimization algorithms\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import STL10\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from unittest.mock import patch\n",
        "from torchvision.models import inception_v3\n",
        "from torchvision.models import vgg19\n",
        "from torchinfo import summary\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set computation device to GPU if available\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"Running on: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JqbxpEjf0q9",
      "metadata": {
        "id": "-JqbxpEjf0q9"
      },
      "source": [
        "**This notebook runs and saves all plots and tables per architecture so that running this model can be split amongst the group**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Ackm0_5f0Hn",
      "metadata": {
        "id": "-Ackm0_5f0Hn"
      },
      "outputs": [],
      "source": [
        "current_architecture = 'resnet'  # 'vgg' or 'inception'\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "esSFs6IY_RVT",
      "metadata": {
        "id": "esSFs6IY_RVT"
      },
      "source": [
        "## 1. Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65cb2dd",
      "metadata": {
        "id": "b65cb2dd"
      },
      "source": [
        "Data loader for CIFAR-10 with resizing to 224x224 to match pretrained ResNet input expectations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ace8b9",
      "metadata": {
        "id": "86ace8b9"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    trainset = STL10(root='./data', split='train', download=True, transform=transform)\n",
        "    testset = STL10(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CX_1IzxZ_lDV",
      "metadata": {
        "id": "CX_1IzxZ_lDV"
      },
      "source": [
        "## 2. Building Attention Enhanced Convolutional Neural Network (CNN) models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe18d947",
      "metadata": {
        "id": "fe18d947"
      },
      "source": [
        "#### Channel Attention Module\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h0Ntl9RK-L7c",
      "metadata": {
        "id": "h0Ntl9RK-L7c"
      },
      "outputs": [],
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, channel_activation_type='sigmoid', channel_pool_weight=\"yes\"):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        # Adaptive pooling: compress feature maps to 1x1 by averaging and maxing across channel dimensions\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # Shared MLP: two conv layers simulate a fully connected bottleneck structure\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, kernel_size=1, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, kernel_size=1, bias=False)\n",
        "\n",
        "        # Learnable parameter\n",
        "        self.alpha_raw = nn.Parameter(torch.tensor(0.0))\n",
        "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "        # For condition\n",
        "        self.pool_weight = channel_pool_weight\n",
        "\n",
        "        # Activation functions\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "\n",
        "        # Blend avg_out and max_out based on learnable alpha\n",
        "        if self.pool_weight == \"yes\":\n",
        "            alpha = torch.sigmoid(self.alpha_raw)\n",
        "            out = alpha * avg_out + (1 - alpha) * max_out\n",
        "        else:\n",
        "            out = avg_out + max_out\n",
        "\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0RIRJzjU_ykv",
      "metadata": {
        "id": "0RIRJzjU_ykv"
      },
      "source": [
        "#### Spatial Attention Module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d29d7a",
      "metadata": {
        "id": "91d29d7a"
      },
      "source": [
        "#### Activation Functions to Output Values for Each Channel Position\n",
        "**Sigmoid (default)**: Allows multiple regions to be highlighted. No competition, so the model can attend to several important areas.\n",
        "\n",
        "**Parametric Sigmoid**: The temperature parameter allows adjusting the steepness of the sigmoid. A higher temperature makes it softer, spreading out the attention, while a lower temperature makes it sharper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EHmuu579NxGO",
      "metadata": {
        "id": "EHmuu579NxGO"
      },
      "outputs": [],
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7, spatial_activation_type='sigmoid', spatial_pool_weight=\"yes\"):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "        # Padding must match kernel size to keep dimensions consistent.\n",
        "        # Must ensure that attention map is the same spatial size as the input feature map\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        # 7x7 convolution on 2-channel input (avg + max across channels)\n",
        "        in_channels = 1 if spatial_pool_weight == \"yes\" else 2\n",
        "        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size, padding=padding, bias=False)\n",
        "\n",
        "        # Learnable parameter\n",
        "        if spatial_pool_weight == \"yes\":\n",
        "            self.alpha_raw = nn.Parameter(torch.tensor(0.0))\n",
        "        else:\n",
        "            self.register_parameter('alpha_raw', None)\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        # For condition\n",
        "        self.spatial_activation_type = spatial_activation_type\n",
        "        self.pool_weight = spatial_pool_weight\n",
        "\n",
        "        # Activation functions\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel-wise pooling to summarize across channels\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        # Blend avg_out and max_out based on learnable alpha\n",
        "        if self.pool_weight == \"yes\":\n",
        "            alpha = torch.sigmoid(self.alpha_raw)\n",
        "            out = alpha * avg_out + (1 - alpha) * max_out\n",
        "        else:\n",
        "            out = torch.cat([avg_out, max_out], dim=1)\n",
        "\n",
        "        # Use convolution layer to generate the spatial attention mask\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        # Apply different activation functions\n",
        "        if self.spatial_activation_type == \"parametric_sigmoid\":\n",
        "            out = self.sigmoid(out / self.temperature)\n",
        "\n",
        "        else:\n",
        "            out = self.sigmoid(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be28eb5",
      "metadata": {
        "id": "0be28eb5"
      },
      "source": [
        "### Adding CBAM to CNN architectures\n",
        "\n",
        "This section wraps VGG19, ResNet18, and InceptionV3 with CBAM at early, middle, or late layer locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7f4fde",
      "metadata": {
        "id": "2b7f4fde"
      },
      "outputs": [],
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self, planes, ratio=16, kernel_size=7,\n",
        "                 channel_activation_type='sigmoid', channel_pool_weight=\"yes\",\n",
        "                 spatial_activation_type='sigmoid', spatial_pool_weight=\"yes\"):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(\n",
        "            planes, ratio,\n",
        "            channel_activation_type=channel_activation_type,\n",
        "            channel_pool_weight=channel_pool_weight\n",
        "        )\n",
        "        self.sa = SpatialAttention(\n",
        "            kernel_size,\n",
        "            spatial_activation_type=spatial_activation_type,\n",
        "            spatial_pool_weight=spatial_pool_weight\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.ca(x) # Apply channel attention first\n",
        "        x = x * self.sa(x) # Then apply spatial attention\n",
        "        return x # Return the feature map with attention applied\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0c7064",
      "metadata": {
        "id": "5b0c7064"
      },
      "outputs": [],
      "source": [
        "# Helper function that inserts CBAM into a Sequential model at a specified index\n",
        "\n",
        "def insert_cbam_sequential(module, index, cbam_module):\n",
        "    layers = list(module.children()) # Breaks the existing Sequential into a list of layers\n",
        "    layers.insert(index, cbam_module) # Inserts CBAM at the desired point\n",
        "    return nn.Sequential(*layers)  # Reassemble - (* is the Python unpacking util: turns a list of layers into multiple arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97876edb",
      "metadata": {
        "id": "97876edb"
      },
      "source": [
        "## 3. Model Building\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### ResNet with CBAM:\n",
        "\n",
        "- Custom ResNet18 wrapper that conditionally adds CBAM at different depths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9fcdd8",
      "metadata": {
        "id": "de9fcdd8"
      },
      "outputs": [],
      "source": [
        "class ResNet18_CBAM(nn.Module):\n",
        "    def __init__(self, attention_position=\"late\", num_classes=10, spatial_activation_type='sigmoid', channel_activation_type='sigmoid', channel_pool_weight=\"yes\", spatial_pool_weight=\"yes\"):\n",
        "        super(ResNet18_CBAM, self).__init__()\n",
        "        base = models.resnet18(pretrained=True)  # Load pretrained ResNet18\n",
        "\n",
        "        # Load pretrained ResNet18\n",
        "        self.stem = nn.Sequential(base.conv1, base.bn1, base.relu, base.maxpool)\n",
        "        self.layer1 = base.layer1 # Output: 64 channels\n",
        "        self.layer2 = base.layer2 # Output: 128 channels\n",
        "        self.layer3 = base.layer3 # Output: 256 channels\n",
        "        self.layer4 = base.layer4 # Output: 512 channels\n",
        "        self.avgpool = base.avgpool\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "        # Conditionally attach CBAM to a specific ResNet block depending on the attention_position specified (CBAM is added to the first block in the specified layer group)\n",
        "        # If 'none', this is passed over - we skip adding any attention block\n",
        "        if attention_position == \"early\":\n",
        "            self.layer1[0].cbam = CBAM(64, spatial_activation_type=spatial_activation_type, channel_activation_type=channel_activation_type, channel_pool_weight=channel_pool_weight, spatial_pool_weight=spatial_pool_weight)\n",
        "            self.cbam_layer = self.layer1[0]\n",
        "        elif attention_position == \"middle\":\n",
        "            self.layer2[0].cbam = CBAM(128, spatial_activation_type=spatial_activation_type, channel_activation_type=channel_activation_type, channel_pool_weight=channel_pool_weight, spatial_pool_weight=spatial_pool_weight)\n",
        "            self.cbam_layer = self.layer2[0]\n",
        "        elif attention_position == \"late\":\n",
        "            self.layer4[0].cbam = CBAM(512, spatial_activation_type=spatial_activation_type, channel_activation_type=channel_activation_type, channel_pool_weight=channel_pool_weight, spatial_pool_weight=spatial_pool_weight)\n",
        "            self.cbam_layer = self.layer4[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        # Apply CBAM only if attached\n",
        "        if hasattr(self.layer1[0], 'cbam'):\n",
        "            x = self.layer1[0].cbam(x)\n",
        "        x = self.layer2(x)\n",
        "        if hasattr(self.layer2[0], 'cbam'):\n",
        "            x = self.layer2[0].cbam(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        if hasattr(self.layer4[0], 'cbam'):\n",
        "            x = self.layer4[0].cbam(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Clmal5Ib9_nC",
      "metadata": {
        "id": "Clmal5Ib9_nC"
      },
      "source": [
        "### InceptionV3 with CBAM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bS_DJav63W4j",
      "metadata": {
        "id": "bS_DJav63W4j"
      },
      "outputs": [],
      "source": [
        "class InceptionV3_CBAM(nn.Module):\n",
        "    def __init__(self, attention_position=\"late\", num_classes=10,\n",
        "                 spatial_activation_type='sigmoid', channel_activation_type='sigmoid',\n",
        "                 channel_pool_weight=\"yes\", spatial_pool_weight=\"yes\"):\n",
        "        super(InceptionV3_CBAM, self).__init__()\n",
        "        inception = inception_v3(pretrained=True)\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            inception.Conv2d_3b_1x1,\n",
        "            inception.Conv2d_4a_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        # Clone inception blocks for manual control\n",
        "        self.Mixed_5b = inception.Mixed_5b  # Output: 192\n",
        "        self.Mixed_5c = inception.Mixed_5c\n",
        "        self.Mixed_5d = inception.Mixed_5d  # Output: 288\n",
        "        self.Mixed_6a = inception.Mixed_6a  # Output: 768\n",
        "        self.Mixed_6b = inception.Mixed_6b\n",
        "        self.Mixed_6c = inception.Mixed_6c\n",
        "        self.Mixed_6d = inception.Mixed_6d\n",
        "        self.Mixed_6e = inception.Mixed_6e\n",
        "        self.Mixed_7a = inception.Mixed_7a\n",
        "        self.Mixed_7b = inception.Mixed_7b\n",
        "        self.Mixed_7c = inception.Mixed_7c\n",
        "\n",
        "        # Insert CBAM\n",
        "        if attention_position == \"early\":\n",
        "            self.Mixed_5b.cbam = CBAM(\n",
        "                planes=192,\n",
        "                spatial_activation_type=spatial_activation_type,\n",
        "                channel_activation_type=channel_activation_type,\n",
        "                channel_pool_weight=channel_pool_weight,\n",
        "                spatial_pool_weight=spatial_pool_weight\n",
        "            )\n",
        "        elif attention_position == \"middle\":\n",
        "            self.Mixed_5d.cbam = CBAM(\n",
        "                planes=288,\n",
        "                spatial_activation_type=spatial_activation_type,\n",
        "                channel_activation_type=channel_activation_type,\n",
        "                channel_pool_weight=channel_pool_weight,\n",
        "                spatial_pool_weight=spatial_pool_weight\n",
        "            )\n",
        "        elif attention_position == \"late\":\n",
        "            self.Mixed_6a.cbam = CBAM(\n",
        "                planes=768,\n",
        "                spatial_activation_type=spatial_activation_type,\n",
        "                channel_activation_type=channel_activation_type,\n",
        "                channel_pool_weight=channel_pool_weight,\n",
        "                spatial_pool_weight=spatial_pool_weight\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"attention_position must be 'early', 'middle', or 'late'\")\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        # Manually forward through inception blocks with CBAM checks\n",
        "        x = self.Mixed_5b(x)\n",
        "        if hasattr(self.Mixed_5b, 'cbam'):\n",
        "            x = self.Mixed_5b.cbam(x)\n",
        "\n",
        "        x = self.Mixed_5c(x)\n",
        "        x = self.Mixed_5d(x)\n",
        "        if hasattr(self.Mixed_5d, 'cbam'):\n",
        "            x = self.Mixed_5d.cbam(x)\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        if hasattr(self.Mixed_6a, 'cbam'):\n",
        "            x = self.Mixed_6a.cbam(x)\n",
        "\n",
        "        x = self.Mixed_6b(x)\n",
        "        x = self.Mixed_6c(x)\n",
        "        x = self.Mixed_6d(x)\n",
        "        x = self.Mixed_6e(x)\n",
        "        x = self.Mixed_7a(x)\n",
        "        x = self.Mixed_7b(x)\n",
        "        x = self.Mixed_7c(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kdDabxEB-E1K",
      "metadata": {
        "id": "kdDabxEB-E1K"
      },
      "source": [
        "### VGG-19 with CBAM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "docTuFx73min",
      "metadata": {
        "id": "docTuFx73min"
      },
      "outputs": [],
      "source": [
        "class VGG19_CBAM(nn.Module):\n",
        "    def __init__(self, attention_position=\"late\", num_classes=10,\n",
        "                 spatial_activation_type='sigmoid', channel_activation_type='sigmoid',\n",
        "                 channel_pool_weight=\"yes\", spatial_pool_weight=\"yes\"):\n",
        "        super(VGG19_CBAM, self).__init__()\n",
        "\n",
        "        # Load pretrained VGG19 and extract features\n",
        "        vgg = vgg19(pretrained=True)\n",
        "        features = list(vgg.features.children())\n",
        "\n",
        "        # Define CBAM insert point\n",
        "        if attention_position == \"early\":\n",
        "            insert_idx = 5\n",
        "        elif attention_position == \"middle\":\n",
        "            insert_idx = 20\n",
        "        elif attention_position == \"late\":\n",
        "            insert_idx = 30\n",
        "\n",
        "        # Find last Conv2d before insert_idx to get correct out_channels\n",
        "        for i in range(insert_idx - 1, -1, -1):\n",
        "            if isinstance(features[i], nn.Conv2d):\n",
        "                planes = features[i].out_channels\n",
        "                break\n",
        "\n",
        "        # Insert CBAM module\n",
        "        cbam = CBAM(planes,\n",
        "                    spatial_activation_type=spatial_activation_type,\n",
        "                    channel_activation_type=channel_activation_type,\n",
        "                    channel_pool_weight=channel_pool_weight,\n",
        "                    spatial_pool_weight=spatial_pool_weight)\n",
        "\n",
        "        self.features = nn.Sequential(*features[:insert_idx], cbam, *features[insert_idx:])\n",
        "\n",
        "        # Classifier\n",
        "        self.avgpool = vgg.avgpool\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ObmpWkH49yTh",
      "metadata": {
        "id": "ObmpWkH49yTh"
      },
      "source": [
        "## 4. Model Summary\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ResNet Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6OCEBfNz-PaQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OCEBfNz-PaQ",
        "outputId": "20311841-f71a-4b93-e959-da328f1c628a"
      },
      "outputs": [],
      "source": [
        "model = ResNet18_CBAM(attention_position=\"late\", num_classes=10)\n",
        "summary(model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TJmveS9f-UFl",
      "metadata": {
        "id": "TJmveS9f-UFl"
      },
      "source": [
        "### InceptionV3 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IQi7-Z11-Unc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQi7-Z11-Unc",
        "outputId": "932cf7dd-e7f8-4a52-d3be-617a683ffde4"
      },
      "outputs": [],
      "source": [
        "model = InceptionV3_CBAM(attention_position=\"late\", num_classes=10)\n",
        "summary(model, input_size=(1, 3, 299, 299))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8umusToA-Pt1",
      "metadata": {
        "id": "8umusToA-Pt1"
      },
      "source": [
        "### VGG-19 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GwJlTRvX9Y5S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwJlTRvX9Y5S",
        "outputId": "50961c53-d507-4776-fee0-ba316f24a74b"
      },
      "outputs": [],
      "source": [
        "model = VGG19_CBAM(attention_position=\"late\", num_classes=10)\n",
        "summary(model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca69eb5",
      "metadata": {
        "id": "cca69eb5"
      },
      "source": [
        "Function that takes the architecture and position where attention is to be added. `num_classes` is always 10 because that is how many categories CIFAR10 has"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f154e8b",
      "metadata": {
        "id": "6f154e8b"
      },
      "outputs": [],
      "source": [
        "def get_attention_model(base='resnet', position='middle', num_classes=10, spatial_activation_type = 'sigmoid', channel_activation_type='sigmoid', channel_pool_weight=\"yes\", spatial_pool_weight=\"yes\"):\n",
        "    if base == 'resnet':\n",
        "        return ResNet18_CBAM(attention_position=position, num_classes=num_classes, spatial_activation_type=spatial_activation_type, channel_activation_type=channel_activation_type, channel_pool_weight=channel_pool_weight, spatial_pool_weight=spatial_pool_weight)\n",
        "    elif base == 'vgg':\n",
        "        return VGG19_CBAM(attention_position=position, num_classes=num_classes, spatial_activation_type=spatial_activation_type, channel_activation_type=channel_activation_type, channel_pool_weight=channel_pool_weight, spatial_pool_weight=spatial_pool_weight)\n",
        "    elif base == 'inception':\n",
        "        return InceptionV3_CBAM(attention_position=position, num_classes=num_classes, spatial_activation_type=spatial_activation_type, channel_activation_type=channel_activation_type, channel_pool_weight=channel_pool_weight, spatial_pool_weight=spatial_pool_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad372f60",
      "metadata": {
        "id": "ad372f60"
      },
      "source": [
        "## 5. Training and Evaluation\n",
        "\n",
        "Includes Top-1, Top-5, Average Precision and gradient flow plot & logging alpha values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59e5770",
      "metadata": {
        "id": "f59e5770"
      },
      "outputs": [],
      "source": [
        "def train_model(model, trainloader, testloader, num_classes=10, epochs=epochs, lr=0.0001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    grad_norms = []\n",
        "    alpha_log = []\n",
        "    spatial_alpha_log = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        total_norm = 0.0\n",
        "\n",
        "        for inputs, labels in tqdm(trainloader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            norm = sum(p.grad.data.norm(2).item() for p in model.parameters() if p.grad is not None)\n",
        "            total_norm += norm\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        grad_norms.append(total_norm / len(trainloader))\n",
        "\n",
        "        # Evaluation\n",
        "        top1, top5, ap = evaluate_model(model, testloader, num_classes)\n",
        "        print(f\"Epoch {epoch+1}: Loss={running_loss:.4f}, GradNorm={grad_norms[-1]:.4f}, Top1={top1:.2f}%, Top5={top5:.2f}%, AP={ap:.4f}\")\n",
        "\n",
        "        # Track alphas if CBAM present\n",
        "        if hasattr(model, 'cbam_layer') and hasattr(model.cbam_layer, 'cbam'):\n",
        "            ca = model.cbam_layer.cbam.ca\n",
        "            sa = model.cbam_layer.cbam.sa\n",
        "\n",
        "            alpha_val = torch.sigmoid(ca.alpha_raw).item() if ca.alpha_raw is not None else None\n",
        "            spatial_val = torch.sigmoid(sa.alpha_raw).item() if sa.alpha_raw is not None else None\n",
        "\n",
        "            if alpha_val is not None:\n",
        "                alpha_log.append(alpha_val)\n",
        "                print(f\"Learned channel alpha at epoch {epoch + 1}: {alpha_val:.4f}\")\n",
        "            if spatial_val is not None:\n",
        "                spatial_alpha_log.append(spatial_val)\n",
        "                print(f\"Learned spatial alpha at epoch {epoch + 1}: {spatial_val:.4f}\")\n",
        "\n",
        "        if top1 > best_acc:\n",
        "            best_acc = top1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # Plot channel & spatial alpha (if any)\n",
        "    if alpha_log or spatial_alpha_log:\n",
        "        plt.figure()\n",
        "        if alpha_log:\n",
        "            plt.plot(range(1, len(alpha_log)+1), alpha_log, label='Channel alpha')\n",
        "        if spatial_alpha_log:\n",
        "            plt.plot(range(1, len(spatial_alpha_log)+1), spatial_alpha_log, label='Spatial alpha')\n",
        "        plt.title(\"Learned Alpha Over Epochs\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Alpha Value\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot gradient norm\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epochs+1), grad_norms, marker='o', label='Gradient Norm')\n",
        "    plt.title(\"Gradient Norm per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Gradient L2 Norm\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model, {\n",
        "        'top1': best_acc,\n",
        "        'grad_norms': grad_norms,\n",
        "        'alpha_log': alpha_log,\n",
        "        'spatial_alpha_log': spatial_alpha_log\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "awB5Lb9XJE0l",
      "metadata": {
        "id": "awB5Lb9XJE0l"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testloader, num_classes=10):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    top5_correct = 0\n",
        "    total = 0\n",
        "    all_probs = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            top5 = torch.topk(outputs, 5, dim=1)[1]\n",
        "            top5_correct += sum([labels[i] in top5[i] for i in range(labels.size(0))])\n",
        "            all_probs.append(probs.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    top1_acc = correct / total * 100\n",
        "    top5_acc = top5_correct / total * 100\n",
        "    ap_score = compute_average_precision(all_targets, all_probs)\n",
        "    return top1_acc, top5_acc, ap_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ab6c9d",
      "metadata": {
        "id": "55ab6c9d"
      },
      "outputs": [],
      "source": [
        "def compute_average_precision(all_targets, all_probs):\n",
        "    num_classes=10\n",
        "\n",
        "    # Concatenate tensors\n",
        "    y_true = torch.cat(all_targets).cpu().numpy()\n",
        "    y_score = torch.cat(all_probs).cpu().numpy()\n",
        "\n",
        "    # Binarize targets (e.g., [3] â†’ [0,0,0,1,0,0,...])\n",
        "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
        "\n",
        "    # Check shape match\n",
        "    assert y_true_bin.shape == y_score.shape, f\"Shape mismatch: {y_true_bin.shape} vs {y_score.shape}\"\n",
        "\n",
        "    # Compute macro-average precision\n",
        "    ap = average_precision_score(y_true_bin, y_score, average='macro')\n",
        "\n",
        "    print(\"y_true_bin shape:\", y_true_bin.shape)  # Should be [N, 10]\n",
        "    print(\"y_score shape:\", y_score.shape)        # Should be [N, 10]\n",
        "    print(\"y_score sample:\", y_score[0])\n",
        "    print(\"y_true one-hot:\", y_true_bin[0])\n",
        "\n",
        "    return ap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea38abaf",
      "metadata": {
        "id": "ea38abaf"
      },
      "source": [
        "### Experimenting with different combinations\n",
        "\n",
        "This cell runs experiments for all **9 combinations** :\n",
        "\n",
        "* Attention used : \"Yes\", \"No\"\n",
        "* Spatial Activations = \"sigmoid\", \"parametric_sigmoid\"\n",
        "* Channel Activation = \"sigmoid\"\n",
        "* Pool Weight Options in Channel = \"no\", \"yes\"\n",
        "* Pool Weight Options in spatial = \"no\", \"yes\"\n",
        "\n",
        "2 * 2 * 2 + 1 = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09862558",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "09862558",
        "outputId": "acec889c-7325-4b0c-cbe1-3e7d2b3b2210"
      },
      "outputs": [],
      "source": [
        "positions = ['late', None]\n",
        "spatial_activations = ['sigmoid', 'parametric_sigmoid']\n",
        "pool_weight_options = ['no','yes']\n",
        "results = []\n",
        "num_classes = 10\n",
        "\n",
        "trainloader, testloader = get_dataloaders()\n",
        "\n",
        "for pos in positions:\n",
        "    # Handle vanilla model outside inner parameter loops\n",
        "    if pos is None:\n",
        "        print(f\"\\nRunning: {current_architecture.upper()} | Vanilla\")\n",
        "\n",
        "        # Load vanilla model\n",
        "        if current_architecture == 'vgg':\n",
        "            model = models.vgg19(pretrained=True)\n",
        "            model.classifier[-1] = nn.Linear(4096, num_classes)\n",
        "        elif current_architecture == 'resnet':\n",
        "            model = models.resnet18(pretrained=True)\n",
        "            model.fc = nn.Linear(512, num_classes)\n",
        "        elif current_architecture == 'inception':\n",
        "            model = models.inception_v3(pretrained=True)\n",
        "            model.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "        model = model.to(device)\n",
        "        trained_model, _ = train_model(model, trainloader, testloader, num_classes=num_classes, epochs=epochs)\n",
        "        top1, top5, ap = evaluate_model(trained_model, testloader, num_classes)\n",
        "\n",
        "        results.append({\n",
        "            'architecture': current_architecture,\n",
        "            'attention': 'none',\n",
        "            'spatial_act': 'n/a',\n",
        "            'channel_act': 'n/a',\n",
        "            'channel_pool': 'n/a',\n",
        "            'spatial_pool': 'n/a',\n",
        "            'top1': top1,\n",
        "            'top5': top5,\n",
        "            'ap': ap\n",
        "        })\n",
        "        continue  # Skip remaining parameters for this position\n",
        "\n",
        "    # Process attention models with all parameter combinations\n",
        "    for spat_act in spatial_activations:\n",
        "        for chan_pool in pool_weight_options:\n",
        "            for spat_pool in pool_weight_options:\n",
        "                print(f\"\\nRunning: {current_architecture.upper()} | Attention: {pos}\")\n",
        "                print(f\"Params: S-Act={spat_act}, C-Pool={chan_pool}, S-Pool={spat_pool}\")\n",
        "\n",
        "                model = get_attention_model(\n",
        "                    base=current_architecture,\n",
        "                    position=pos,\n",
        "                    num_classes=num_classes,\n",
        "                    spatial_activation_type=spat_act,\n",
        "                    channel_pool_weight=chan_pool,\n",
        "                    spatial_pool_weight=spat_pool\n",
        "                )\n",
        "                model = model.to(device)\n",
        "\n",
        "                if chan_pool == \"no\" and spat_pool == \"no\":\n",
        "                    with patch(\"matplotlib.pyplot.show\"):\n",
        "                        trained_model, _ = train_model(model, trainloader, testloader, num_classes=num_classes, epochs=epochs)\n",
        "                else:\n",
        "                    trained_model, _ = train_model(model, trainloader, testloader, num_classes=num_classes, epochs=epochs)\n",
        "\n",
        "                top1, top5, ap = evaluate_model(trained_model, testloader, num_classes)\n",
        "\n",
        "                results.append({\n",
        "                    'architecture': current_architecture,\n",
        "                    'attention': pos,\n",
        "                    'spatial_act': spat_act,\n",
        "                    'channel_act': 'sigmoid',\n",
        "                    'channel_pool': chan_pool,\n",
        "                    'spatial_pool': spat_pool,\n",
        "                    'top1': top1,\n",
        "                    'top5': top5,\n",
        "                    'ap': ap\n",
        "                })\n",
        "\n",
        "# Create summary table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\nAll experiments complete. Summary:\")\n",
        "display(df_results)\n",
        "\n",
        "# Generate LaTeX table\n",
        "latex_table = df_results.to_latex(index=False, caption=f\"{current_architecture.upper()} CBAM Results\", label=f\"tab:{current_architecture}_results\")\n",
        "print(latex_table)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
